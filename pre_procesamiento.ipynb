{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "pre-procesamiento.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merrecalde/taller-cuenca/blob/master/pre_procesamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhxKZmKujgH_",
        "colab_type": "text"
      },
      "source": [
        "# Notebook: pre-procesamiento.ipynb\n",
        "#### Se ejemplifican algunas de las tareas de pre-procesamiento descriptas en la clase 2: Pre-proesamiento de textos del curso \"Enfoques Clásicos y Neuronales a la Minería de Texto\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSCm5V2Ckt0E",
        "colab_type": "text"
      },
      "source": [
        "## 1) Partición de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VGZcxO3RFq8",
        "colab_type": "code",
        "outputId": "54de6638-f643-4bf5-c617-b7521478b0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import re\n",
        "\n",
        "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very \n",
        "hopeful tone though), 'I won't have any pepper in my kitchen AT \n",
        "ALL. Soup does very well without--Maybe it's always pepper that \n",
        "makes people hot-tempered,'\"\"\"\n",
        "\n",
        "print(raw.split()) #usando split de strings como herramienta\n",
        "print()\n",
        "print(re.split(r' ', raw)) #con expresiones regulares (ojo) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'\"]\n",
            "\n",
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', '\\nhopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', '\\nALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', '\\nmakes', 'people', \"hot-tempered,'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6_qfMWBRFrB",
        "colab_type": "code",
        "outputId": "7b0028b2-f43e-4256-f761-2a2c60da2003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(re.split(r'[ \\t\\n]+', raw))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDy-ZsnLRFrF",
        "colab_type": "code",
        "outputId": "91225bb7-8902-44ce-9acc-9c8a3cb83516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(re.split(r'\\s+', raw))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SbPjgWMRFrI",
        "colab_type": "code",
        "outputId": "a956b2a1-b4dc-4575-a537-dccea3061eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(re.split(r'\\W+', raw))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9rjn4BZRFrL",
        "colab_type": "code",
        "outputId": "7da81a15-accb-4df3-fa3c-3de34e6731ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(re.findall(r'\\w+', raw))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn-95lBGRFrP",
        "colab_type": "code",
        "outputId": "ee8c2f77-2d4f-4996-c432-92fc932d1014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(re.findall(r'\\w+|\\S\\w*', raw))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MmbnSxstVv2",
        "colab_type": "text"
      },
      "source": [
        "## 2) Filtrado de palabras\n",
        "Se ejemplifica en otras notebooks junto con la normalización o la vectorización de documentos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBKWlNbFtyNd",
        "colab_type": "text"
      },
      "source": [
        "## 3) Normalizaciones de palabras\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D6NevHauBSV",
        "colab_type": "text"
      },
      "source": [
        "### 3.1) Lematización y truncado\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnPRf9dqRFrV",
        "colab_type": "code",
        "outputId": "a9e06058-926e-4b7e-9fc0-165c0e80fc12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "\n",
        "# cargar el modelo del lenguaje inglés de spacy\n",
        "en_nlp = spacy.load('en')\n",
        "doc = u\"I saw there some saws to cut the tree\"\n",
        "# instanciar el \"stemmer\" de Porter de nltk\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "# tokenizar documento con spacy\n",
        "doc_spacy = en_nlp(doc)\n",
        "# imprimir lemas encontrados por spacy\n",
        "print(\"Lematización:\")\n",
        "print([token.lemma_ for token in doc_spacy])\n",
        "# imprimir tokens obtenidos con el stemmer de Porter\n",
        "print(\"Truncado:\")\n",
        "print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lematización:\n",
            "['-PRON-', 'see', 'there', 'some', 'saw', 'to', 'cut', 'the', 'tree']\n",
            "Truncado:\n",
            "['i', 'saw', 'there', 'some', 'saw', 'to', 'cut', 'the', 'tree']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9glIN3YyaRW",
        "colab_type": "text"
      },
      "source": [
        "## 4) Etiquetado\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSqsm4Boyybr",
        "colab_type": "text"
      },
      "source": [
        "### 4.1) Etiquetado de las categorías gramaticales (POS tagging) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Df8TgtRFrg",
        "colab_type": "code",
        "outputId": "0dcf7cba-bc99-40f8-d655-0ae11d01c919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, wordpunct_tokenize\n",
        "text = \"The old building was demolished. Tomorrow, they will begin building a new one\"\n",
        "pos_tag(wordpunct_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('old', 'JJ'),\n",
              " ('building', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('demolished', 'VBN'),\n",
              " ('.', '.'),\n",
              " ('Tomorrow', 'NNP'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('begin', 'VB'),\n",
              " ('building', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('new', 'JJ'),\n",
              " ('one', 'CD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow9d9umkRFrk",
        "colab_type": "code",
        "outputId": "d8e99fa3-e4b4-4696-8222-005ecd026323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "text = \"The old building was demolished. Tomorrow, they will begin building a new one\"\n",
        "pos_tag(word_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('old', 'JJ'),\n",
              " ('building', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('demolished', 'VBN'),\n",
              " ('.', '.'),\n",
              " ('Tomorrow', 'NNP'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('begin', 'VB'),\n",
              " ('building', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('new', 'JJ'),\n",
              " ('one', 'CD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJJwQg2aRFrn",
        "colab_type": "code",
        "outputId": "ff1a1ff8-e9ec-424e-d09d-8c2278cc9426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "text = \"The grand jury commented on a number of other topics.\"\n",
        "pos_tag(wordpunct_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('grand', 'JJ'),\n",
              " ('jury', 'NN'),\n",
              " ('commented', 'VBD'),\n",
              " ('on', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('number', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('other', 'JJ'),\n",
              " ('topics', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyj8BRN8v4N9",
        "colab_type": "text"
      },
      "source": [
        "### 4.2) Desambiguación del sentido de las palabras (WSD)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjWiLOcpRFrq",
        "colab_type": "code",
        "outputId": "2ca70bfa-4543-4b4f-8d1e-10b76014ec0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets('bass')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('bass.n.01'),\n",
              " Synset('bass.n.02'),\n",
              " Synset('bass.n.03'),\n",
              " Synset('sea_bass.n.01'),\n",
              " Synset('freshwater_bass.n.01'),\n",
              " Synset('bass.n.06'),\n",
              " Synset('bass.n.07'),\n",
              " Synset('bass.n.08'),\n",
              " Synset('bass.s.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4SsAljsRFrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Natural Language Toolkit: Word Sense Disambiguation Algorithms\n",
        "#\n",
        "# Authors: Liling Tan <alvations@gmail.com>,\n",
        "#          Dmitrijs Milajevs <dimazest@gmail.com>\n",
        "#\n",
        "# Copyright (C) 2001-2018 NLTK Project\n",
        "# URL: <http://nltk.org/>\n",
        "# For license information, see LICENSE.TXT\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "def lesk(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
        "    \"\"\"Return a synset for an ambiguous word in a context.\n",
        "\n",
        "    :param iter context_sentence: The context sentence where the ambiguous word\n",
        "         occurs, passed as an iterable of words.\n",
        "    :param str ambiguous_word: The ambiguous word that requires WSD.\n",
        "    :param str pos: A specified Part-of-Speech (POS).\n",
        "    :param iter synsets: Possible synsets of the ambiguous word.\n",
        "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
        "\n",
        "    This function is an implementation of the original Lesk algorithm (1986) [1].\n",
        "\n",
        "    Usage example::\n",
        "\n",
        "        >>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')\n",
        "        Synset('savings_bank.n.02')\n",
        "\n",
        "    [1] Lesk, Michael. \"Automatic sense disambiguation using machine\n",
        "    readable dictionaries: how to tell a pine cone from an ice cream\n",
        "    cone.\" Proceedings of the 5th Annual International Conference on\n",
        "    Systems Documentation. ACM, 1986.\n",
        "    http://dl.acm.org/citation.cfm?id=318728\n",
        "    \"\"\"\n",
        "\n",
        "    context = set(context_sentence)\n",
        "    if synsets is None:\n",
        "        synsets = wordnet.synsets(ambiguous_word)\n",
        "\n",
        "    if pos:\n",
        "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
        "\n",
        "    if not synsets:\n",
        "        return None\n",
        "\n",
        "    _, sense = max(\n",
        "        (len(context.intersection(ss.definition().split())), ss) for ss in synsets\n",
        "    )\n",
        "\n",
        "    return sense\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiiH-Ix2RFrv",
        "colab_type": "code",
        "outputId": "ea70b04c-beb2-4f5c-843a-851b785d351f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('savings_bank.n.02')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEHuGT8cyTEh",
        "colab_type": "text"
      },
      "source": [
        "### 4.3) Reconocimiento de entidades nombradas (NER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zONJRU4xRFry",
        "colab_type": "code",
        "outputId": "0f2946ec-78ad-47d6-85cb-cf87ef189daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download('treebank')\n",
        "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
        "sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('U.S.', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('one', 'CD'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('few', 'JJ'),\n",
              " ('industrialized', 'VBN'),\n",
              " ('nations', 'NNS'),\n",
              " ('that', 'WDT'),\n",
              " ('*T*-7', '-NONE-'),\n",
              " ('does', 'VBZ'),\n",
              " (\"n't\", 'RB'),\n",
              " ('have', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('higher', 'JJR'),\n",
              " ('standard', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('regulation', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('smooth', 'JJ'),\n",
              " (',', ','),\n",
              " ('needle-like', 'JJ'),\n",
              " ('fibers', 'NNS'),\n",
              " ('such', 'JJ'),\n",
              " ('as', 'IN'),\n",
              " ('crocidolite', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('*T*-1', '-NONE-'),\n",
              " ('are', 'VBP'),\n",
              " ('classified', 'VBN'),\n",
              " ('*-5', '-NONE-'),\n",
              " ('as', 'IN'),\n",
              " ('amphobiles', 'NNS'),\n",
              " (',', ','),\n",
              " ('according', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('Brooke', 'NNP'),\n",
              " ('T.', 'NNP'),\n",
              " ('Mossman', 'NNP'),\n",
              " (',', ','),\n",
              " ('a', 'DT'),\n",
              " ('professor', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('pathlogy', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('University', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('Vermont', 'NNP'),\n",
              " ('College', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('Medicine', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP5VFGJvRFr1",
        "colab_type": "code",
        "outputId": "019af708-c659-437d-e2b4-27fa7ff24677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "print(nltk.ne_chunk(sent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  The/DT\n",
            "  (GPE U.S./NNP)\n",
            "  is/VBZ\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  few/JJ\n",
            "  industrialized/VBN\n",
            "  nations/NNS\n",
            "  that/WDT\n",
            "  *T*-7/-NONE-\n",
            "  does/VBZ\n",
            "  n't/RB\n",
            "  have/VB\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  standard/NN\n",
            "  of/IN\n",
            "  regulation/NN\n",
            "  for/IN\n",
            "  the/DT\n",
            "  smooth/JJ\n",
            "  ,/,\n",
            "  needle-like/JJ\n",
            "  fibers/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  crocidolite/NN\n",
            "  that/WDT\n",
            "  *T*-1/-NONE-\n",
            "  are/VBP\n",
            "  classified/VBN\n",
            "  *-5/-NONE-\n",
            "  as/IN\n",
            "  amphobiles/NNS\n",
            "  ,/,\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
            "  ,/,\n",
            "  a/DT\n",
            "  professor/NN\n",
            "  of/IN\n",
            "  pathlogy/NN\n",
            "  at/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION University/NNP)\n",
            "  of/IN\n",
            "  (PERSON Vermont/NNP College/NNP)\n",
            "  of/IN\n",
            "  (GPE Medicine/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}